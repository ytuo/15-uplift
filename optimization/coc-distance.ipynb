{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e413f3df",
   "metadata": {},
   "source": [
    "# Distance between CoCs\n",
    "A factory can be placed in a CoC and serves two functions:\n",
    "\n",
    "1. Provide shelter to people experiencing homelessness (PEH) in that CoC.\n",
    "2. Serve as a source for other temporary housing units (THUs) that can be deployed to other CoCs.\n",
    "\n",
    "To determine the optimal deployment of factories, we require the distance between each pair of CoCs; from this distance we can calculate a transportation cost that will be used in the optimization framework to strategically deploy THUs and factories. In this work, we consider two ways to calculate the distance between CoCs:\n",
    "\n",
    "1. Haversine distance\n",
    "2. Road network distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41460d3a",
   "metadata": {},
   "source": [
    "## Haversine distance\n",
    "The hversine distance determines the orthodromic distance between two points on a sphere. Given the latitude $\\phi$ and longitude $\\lambda$ of two points, we can calculate the distance according to:\n",
    "$$\n",
    "d = 2r \\sin^{-1}\\left( \\sqrt \\frac{1 - \\cos(\\Delta \\phi) + \\cos(\\phi_1) \\cos(\\phi_2)(1-\\cos(\\Delta \\lambda))}{2} \\right)\n",
    "$$\n",
    "where $r$ is the radius of the Earth (approximately 6,370 km)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d533b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130bce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Earth radius R in miles\n",
    "R = 3958.8\n",
    "def haversine_dist(lat1, lon1, lat2, lon2):\n",
    "    # Convert to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    delta_latitude = lat2_rad - lat1_rad\n",
    "    delta_longitude = lon2_rad - lon1_rad\n",
    "    # sine version    \n",
    "    return 2*R*math.asin(math.sqrt((1 - math.cos(delta_latitude) + math.cos(lat1_rad)*math.cos(lat2_rad)*(1 - math.cos(delta_longitude)))/2))\n",
    "\n",
    "    # arctan version\n",
    "    # a = (1 - math.cos(delta_latitude) + math.cos(lat1_rad)*math.cos(lat2_rad)*(1 - math.cos(delta_longitude)))/2\n",
    "    # return 2*R*math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "\n",
    "# Validation\n",
    "# print(haversine_dist(33.5, -86.67, 30.76, -87.93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3609573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# test: SW\n",
    "state_abbreviations = [\"AZ\", \"CA\", \"CT\", \"ME\", \"MA\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"OK\", \"PA\", \"RI\", \"TX\", \"UT\", \"VT\"]\n",
    "\n",
    "states = [\"Arizona\", \"California\", \"Connecticut\", \"Maine\", \"Massachusetts\", \"Nevada\", \"New_Hampshire\", \"New_Jersey\", \"New_Mexico\", \"New_York\", \"Oklahoma\", \"Pennsylvania\", \"Rhode_Island\", \"Texas\", \"Utah\", \"Vermont\"]\n",
    "\n",
    "# state_abbreviations = [\n",
    "#     \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "#     \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "#     \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "#     \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "#     \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# states = [\n",
    "#     'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', \n",
    "#     'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', \n",
    "#     'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', \n",
    "#     'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', \n",
    "#     'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', \n",
    "#     'New_Hampshire', 'New_Jersey', 'New_Mexico', 'New_York', \n",
    "#     'North_Carolina', 'North_Dakota', 'Ohio', 'Oklahoma', 'Oregon', \n",
    "#     'Pennsylvania', 'Rhode_Island', 'South_Carolina', 'South_Dakota', \n",
    "#     'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', \n",
    "#     'West_Virginia', 'Wisconsin', 'Wyoming'\n",
    "# ]\n",
    "\n",
    "\n",
    "abbreviation_state_pairs = list(zip(state_abbreviations, states))\n",
    "\n",
    "# Validation\n",
    "# for i in range(len(abbreviation_state_pairs)):\n",
    "#     print(abbreviation_state_pairs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1dd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample processing of .shp\n",
    "# current_dir = os.getcwd()\n",
    "# gdf = gpd.read_file(current_dir + \"\\CoC_GIS_State_Shapefile_MA\\Massachusetts\\MA_500\\MA_500.shp\")\n",
    "# # gdf.info()\n",
    "\n",
    "# # print(len(gdf))\n",
    "\n",
    "# # Compute centroids\n",
    "# centroids = gdf.geometry.centroid\n",
    "\n",
    "# # Extract latitudes and longitudes\n",
    "# longitudes = centroids.x\n",
    "# latitudes = centroids.y\n",
    "\n",
    "# # Compute averages\n",
    "# avg_lon = longitudes.mean()\n",
    "# avg_lat = latitudes.mean()\n",
    "\n",
    "# print(\"Latitude, longitude: \", avg_lat, avg_lon)\n",
    "\n",
    "# # # Get the centroid of the first geometry\n",
    "# # centroid = gdf.geometry.iloc[0].centroid\n",
    "\n",
    "# # # Get latitude and longitude\n",
    "# # longitude = centroid.x\n",
    "# # latitude = centroid.y\n",
    "\n",
    "# # print(\"Latitude, longitude: \", latitude, longitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c7b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Get CoCs for all states\n",
    "cocs = []\n",
    "coc_lon_lat = {}\n",
    "directory = Path.cwd().parent\n",
    "for abbreviation, state in abbreviation_state_pairs:\n",
    "    # Get CoC paths and CoCs per state folder\n",
    "    if state == \"Wyoming\":\n",
    "        coc_path = directory / 'homelessness-prediction' / 'data/coc-shapefiles/2023' / f'CoC_GIS_State_Shapefile_{abbreviation}'\n",
    "    else:\n",
    "        coc_path = directory / 'homelessness-prediction' / 'data/coc-shapefiles/2023' / f'CoC_GIS_State_Shapefile_{abbreviation}' / state\n",
    "    for coc in os.listdir(coc_path):\n",
    "        if coc.startswith(abbreviation + '_'):\n",
    "            cocs.append(coc)\n",
    "            gdf = gpd.read_file(coc_path / coc / f'{coc}.shp')\n",
    "            # Compute centroids (if more than one)\n",
    "            centroids = gdf.geometry.centroid\n",
    "\n",
    "            # Extract latitudes and longitudes (if more than one)\n",
    "            longitudes= centroids.x\n",
    "            latitudes = centroids.y\n",
    "\n",
    "            # Compute average of centroids (if more than one)\n",
    "            avg_lon = longitudes.mean()\n",
    "            avg_lat = latitudes.mean()\n",
    "\n",
    "            coc_lon_lat[coc] = (avg_lon, avg_lat)\n",
    "\n",
    "cocs.sort()\n",
    "# Validation\n",
    "# print(cocs)\n",
    "# print(numCoCs)\n",
    "# print(coc_lon_lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08939da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce distance matrix\n",
    "# Create city distances matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "coc_distance_matrix = pd.DataFrame(columns=[f'{coc}' for coc in cocs])\n",
    "\n",
    "for coc_source, lon_lat_source in coc_lon_lat.items():\n",
    "    coc_distance_arr = []\n",
    "    for coc_dest, lon_lat_dest in coc_lon_lat.items():\n",
    "        distance = haversine_dist(lat1=lon_lat_source[1], lon1=lon_lat_source[0], lat2=lon_lat_dest[1], lon2=lon_lat_dest[0])\n",
    "        # print(coc_source)\n",
    "        # print(coc_dest)\n",
    "        # print(lon_lat_source)\n",
    "        # print(lon_lat_dest)\n",
    "        # print(distance)\n",
    "        coc_distance_arr.append(distance)\n",
    "    coc_distance_matrix.loc[len(coc_distance_matrix)] = coc_distance_arr\n",
    "    # if coc_source == \"AL_503\":\n",
    "    #     break\n",
    "\n",
    "# print(coc_distance_matrix.head())\n",
    "\n",
    "coc_distance_matrix['CoC'] = cocs\n",
    "# coc_distance_matrix.set_index('CoC', inplace=True)\n",
    "\n",
    "# Validation \n",
    "# print(coc_distance_matrix.shape)\n",
    "# print(coc_distance_matrix.head())\n",
    "# print(coc_distance_matrix['AL_504']['AL_505'])\n",
    "# print(coc_distance_matrix.iloc[4,5]) # should be equivalent to previous\n",
    "# print(coc_lon_lat['AL_504'])\n",
    "# print(coc_lon_lat['AL_505'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1968e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# CoC populations \n",
    "# Load the Excel file into a DataFrame\n",
    "directory = Path.cwd().parent\n",
    "# CoC_populations = pd.read_excel(directory / 'homelessness-prediction/coc-homelessness-data/pit_count.xlsx')\n",
    "CoC_populations = pd.read_excel(directory / 'homelessness-prediction/coc-homelessness-data/2007-2024-PIT-Counts-by-CoC.xlsx', sheet_name='2023')\n",
    "\n",
    "# Only keep population column\n",
    "# CoC_populations = CoC_populations[['CoC_Number', 'Overall_Homeless']]\n",
    "CoC_populations = CoC_populations[['CoC_Number', 'Overall Homeless']]\n",
    "# Drop indices\n",
    "CoC_populations = CoC_populations.drop(index=0)\n",
    "\n",
    "# Remove CoCs where there is missing info\n",
    "CoC_populations['CoC_Number'] = CoC_populations['CoC_Number'].str.replace('-', '_', regex=True)\n",
    "CoC_populations = CoC_populations[CoC_populations['CoC_Number'].isin(cocs)]\n",
    "\n",
    "# coc = coc.iloc[1:].reset_index(drop=True)\n",
    "for coc in cocs[:]:\n",
    "    if coc not in CoC_populations['CoC_Number'].values:\n",
    "        cocs.remove(coc)\n",
    "\n",
    "# # Validation\n",
    "# for coc in CoC_populations:\n",
    "#     if coc not in cocs:\n",
    "#         print(coc)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(CoC_populations)\n",
    "\n",
    "# # Label indices by CoC name\n",
    "# CoC_populations.set_index('CoC_Number', inplace=True)\n",
    "\n",
    "# More validation\n",
    "print(len(cocs))\n",
    "# print(cocs)\n",
    "print(len(CoC_populations))\n",
    "# print(CoC_populations.head())\n",
    "# print(CoC_populations.iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f28dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "['AZ_500', 'AZ_501', 'AZ_502', 'CA_500', 'CA_501', 'CA_502', 'CA_503', 'CA_504', 'CA_505', 'CA_506', 'CA_507', 'CA_508', 'CA_509', 'CA_510', 'CA_511', 'CA_512', 'CA_513', 'CA_514', 'CA_515', 'CA_516', 'CA_517', 'CA_518', 'CA_519', 'CA_520', 'CA_521', 'CA_522', 'CA_523', 'CA_524', 'CA_525', 'CA_526', 'CA_527', 'CA_529', 'CA_530', 'CA_531', 'CA_600', 'CA_601', 'CA_602', 'CA_603', 'CA_604', 'CA_606', 'CA_607', 'CA_608', 'CA_609', 'CA_611', 'CA_612', 'CA_613', 'CA_614', 'CT_503', 'CT_505', 'MA_500', 'MA_502', 'MA_503', 'MA_504', 'MA_505', 'MA_506', 'MA_507', 'MA_509', 'MA_511', 'MA_515', 'MA_516', 'ME_500', 'NH_500', 'NH_501', 'NH_502', 'NJ_500', 'NJ_501', 'NJ_502', 'NJ_503', 'NJ_504', 'NJ_506', 'NJ_507', 'NJ_508', 'NJ_509', 'NJ_510', 'NJ_511', 'NJ_512', 'NJ_513', 'NJ_514', 'NJ_515', 'NJ_516', 'NM_500', 'NM_501', 'NV_500', 'NV_501', 'NV_502', 'NY_500', 'NY_501', 'NY_503', 'NY_505', 'NY_507', 'NY_508', 'NY_510', 'NY_511', 'NY_512', 'NY_513', 'NY_514', 'NY_518', 'NY_519', 'NY_520', 'NY_522', 'NY_523', 'NY_525', 'NY_600', 'NY_601', 'NY_602', 'NY_603', 'NY_604', 'NY_606', 'NY_608', 'OK_500', 'OK_501', 'OK_502', 'OK_503', 'OK_504', 'OK_505', 'OK_506', 'OK_507', 'PA_500', 'PA_501', 'PA_502', 'PA_503', 'PA_504', 'PA_505', 'PA_506', 'PA_508', 'PA_509', 'PA_510', 'PA_511', 'PA_512', 'PA_600', 'PA_601', 'PA_603', 'PA_605', 'RI_500', 'TX_500', 'TX_503', 'TX_600', 'TX_601', 'TX_603', 'TX_604', 'TX_607', 'TX_611', 'TX_624', 'TX_700', 'TX_701', 'UT_500', 'UT_503', 'UT_504', 'VT_500', 'VT_501']\n",
      "150\n",
      "   CoC_Number  microhome_cost\n",
      "10     AZ_502    1.558584e+07\n",
      "11     AZ_500    9.923120e+06\n",
      "12     AZ_501    1.056527e+07\n",
      "17     CA_521    2.171147e+07\n",
      "18     CA_519    1.496039e+07\n",
      "CoC_Number                 AZ_502\n",
      "microhome_cost    15585836.084747\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rental price\n",
    "# Load the Excel file into a DataFrame\n",
    "directory = Path.cwd().parent\n",
    "factory_estimate = pd.read_excel(directory / 'homelessness-prediction/coc-homelessness-data/coc_summary_6.xlsx', sheet_name='2023')\n",
    "\n",
    "# Only keep population column\n",
    "factory_estimate = factory_estimate[['CoC_Number', 'median_home_value']]\n",
    "# Drop indices\n",
    "factory_estimate = factory_estimate.drop(index=0)\n",
    "\n",
    "# Remove CoCs where there is missing info\n",
    "factory_estimate['CoC_Number'] = factory_estimate['CoC_Number'].str.replace('-', '_', regex=True)\n",
    "factory_estimate = factory_estimate[factory_estimate['CoC_Number'].isin(cocs)]\n",
    "\n",
    "# coc = coc.iloc[1:].reset_index(drop=True)\n",
    "for coc in cocs[:]:\n",
    "    if coc not in factory_estimate['CoC_Number'].values:\n",
    "        cocs.remove(coc)\n",
    "\n",
    "# assume factory cost is 3 * cost of median_home_value\n",
    "factory_cost_scalar = 36\n",
    "factory_estimate['microhome_cost'] = factory_estimate['median_home_value'] * factory_cost_scalar\n",
    "\n",
    "# # Validation\n",
    "# for coc in factory_estimate:\n",
    "#     if coc not in cocs:\n",
    "#         print(coc)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(factory_estimate)\n",
    "\n",
    "# # Label indices by CoC name\n",
    "# factory_estimate.set_index('CoC_Number', inplace=True)\n",
    "\n",
    "factory_estimate = factory_estimate.drop(columns=['median_home_value'])\n",
    "\n",
    "\n",
    "# More validation\n",
    "print(len(cocs))\n",
    "print(cocs)\n",
    "print(len(factory_estimate))\n",
    "print(factory_estimate.head())\n",
    "print(factory_estimate.iloc[0])\n",
    "\n",
    "numCoCs = len(cocs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6735239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "coc_distance_matrix.to_csv('coc_distance_matrix.csv', index=False) # index=False prevents writing row numbers\n",
    "CoC_populations.to_csv('Coc_populations.csv', index=False)  \n",
    "factory_estimate.to_csv('factory_estimate.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11275636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4200e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
